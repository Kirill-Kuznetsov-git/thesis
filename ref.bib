@ARTICLE{Imagen,
  title         = "Photorealistic text-to-image diffusion models with deep
                   language understanding",
  author        = "Saharia, Chitwan and Chan, William and Saxena, Saurabh and
                   Li, Lala and Whang, Jay and Denton, Emily and Ghasemipour,
                   Seyed Kamyar Seyed and Ayan, Burcu Karagol and Mahdavi, S
                   Sara and Lopes, Rapha Gontijo and Salimans, Tim and Ho,
                   Jonathan and Fleet, David J and Norouzi, Mohammad",
  month         =  may,
  year          =  2022,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2205.11487"
}
@ARTICLE{PixArt,
  title         = "{PixArt-$\alpha$}: Fast training of Diffusion Transformer
                   for photorealistic text-to-image synthesis",
  author        = "Chen, Junsong and Yu, Jincheng and Ge, Chongjian and Yao,
                   Lewei and Xie, Enze and Wu, Yue and Wang, Zhongdao and Kwok,
                   James and Luo, Ping and Lu, Huchuan and Li, Zhenguo",
  month         =  sep,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2310.00426"
}

@ARTICLE{Latent_Diffusion,
  title         = "High-resolution image synthesis with latent diffusion models",
  author        = "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik
                   and Esser, Patrick and Ommer, Bj{\"o}rn",
  month         =  dec,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2112.10752"
}

@ARTICLE{Casceded_Diffusion,
  title         = "Cascaded diffusion models for high fidelity image generation",
  author        = "Ho, Jonathan and Saharia, Chitwan and Chan, William and
                   Fleet, David J and Norouzi, Mohammad and Salimans, Tim",
  month         =  may,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2106.15282"
}

@ARTICLE{Stable_Duffusion,
  title         = "High-resolution image synthesis with latent diffusion models",
  author        = "Rombach, Robin and Blattmann, Andreas and Lorenz, Dominik
                   and Esser, Patrick and Ommer, Bj{\"o}rn",
  month         =  dec,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2112.10752"
}

@ARTICLE{FID,
  title         = "{GANs} trained by a two time-scale update rule converge to a
                   local Nash equilibrium",
  author        = "Heusel, Martin and Ramsauer, Hubert and Unterthiner, Thomas
                   and Nessler, Bernhard and Hochreiter, Sepp",
  month         =  jun,
  year          =  2017,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "1706.08500"
}
@ARTICLE{CLIPScore,
  title         = "{CLIPScore}: A reference-free evaluation metric for image
                   captioning",
  author        = "Hessel, Jack and Holtzman, Ari and Forbes, Maxwell and Bras,
                   Ronan Le and Choi, Yejin",
  month         =  apr,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2104.08718"
}

@ARTICLE{CLIP,
  title         = "Learning transferable visual models from natural language
                   supervision",
  author        = "Radford, Alec and Kim, Jong Wook and Hallacy, Chris and
                   Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and
                   Sastry, Girish and Askell, Amanda and Mishkin, Pamela and
                   Clark, Jack and Krueger, Gretchen and Sutskever, Ilya",
  month         =  feb,
  year          =  2021,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2103.00020"
}

@ARTICLE{BLIP,
  title         = "{BLIP}: Bootstrapping language-image pre-training for
                   unified vision-language understanding and generation",
  author        = "Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven",
  month         =  jan,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2201.12086"
}

@ARTICLE{KD_CLIP,
  title         = "Rethinking {FID}: Towards a better evaluation metric for
                   image generation",
  author        = "Jayasumana, Sadeep and Ramalingam, Srikumar and Veit,
                   Andreas and Glasner, Daniel and Chakrabarti, Ayan and Kumar,
                   Sanjiv",
  month         =  nov,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by-nc-sa/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2401.09603"
}

@ARTICLE{FID_Med,
  title         = "Importance of feature extraction in the calculation of
                   Fr\textbackslash'echet distance for medical imaging",
  author        = "Woodland, Mckell and Taie, Mais Al and Silva, Jessica
                   Albuquerque Marques and Eltaher, Mohamed and Mohn, Frank and
                   Shieh, Alexander and Castelo, Austin and Kundu, Suprateek
                   and Yung, Joshua P and Patel, Ankit B and Brock, Kristy K",
  month         =  nov,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2311.13717"
}

@ARTICLE{FD_DINOv2,
  title         = "Exposing flaws of generative model evaluation metrics and
                   their unfair treatment of diffusion models",
  author        = "Stein, George and Cresswell, Jesse C and Hosseinzadeh, Rasa
                   and Sui, Yi and Ross, Brendan Leigh and Villecroze, Valentin
                   and Liu, Zhaoyan and Caterini, Anthony L and Taylor, J Eric
                   T and Loaiza-Ganem, Gabriel",
  month         =  jun,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2306.04675"
}

@ARTICLE{DINOv2,
  title         = "{DINOv2}: Learning robust visual features without
                   supervision",
  author        = "Oquab, Maxime and Darcet, Timoth{\'e}e and Moutakanni,
                   Th{\'e}o and Vo, Huy and Szafraniec, Marc and Khalidov,
                   Vasil and Fernandez, Pierre and Haziza, Daniel and Massa,
                   Francisco and El-Nouby, Alaaeldin and Assran, Mahmoud and
                   Ballas, Nicolas and Galuba, Wojciech and Howes, Russell and
                   Huang, Po-Yao and Li, Shang-Wen and Misra, Ishan and Rabbat,
                   Michael and Sharma, Vasu and Synnaeve, Gabriel and Xu, Hu
                   and Jegou, Herv{\'e} and Mairal, Julien and Labatut, Patrick
                   and Joulin, Armand and Bojanowski, Piotr",
  month         =  apr,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.07193"
}

@ARTICLE{FD,
  title     = "The Fr{\'e}chet distance between multivariate normal
               distributions",
  author    = "Dowson, D C and Landau, B V",
  journal   = "J. Multivar. Anal.",
  publisher = "Elsevier BV",
  volume    =  12,
  number    =  3,
  pages     = "450--455",
  month     =  sep,
  year      =  1982,
  copyright = "https://www.elsevier.com/open-access/userlicense/1.0/",
  language  = "en"
}

@ARTICLE{InceptionV3,
  title         = "Rethinking the inception architecture for computer vision",
  author        = "Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey
                   and Shlens, Jonathon and Wojna, Zbigniew",
  abstract      = "Convolutional networks are at the core of most
                   state-of-the-art computer vision solutions for a wide
                   variety of tasks. Since 2014 very deep convolutional
                   networks started to become mainstream, yielding substantial
                   gains in various benchmarks. Although increased model size
                   and computational cost tend to translate to immediate
                   quality gains for most tasks (as long as enough labeled data
                   is provided for training), computational efficiency and low
                   parameter count are still enabling factors for various use
                   cases such as mobile vision and big-data scenarios. Here we
                   explore ways to scale up networks in ways that aim at
                   utilizing the added computation as efficiently as possible
                   by suitably factorized convolutions and aggressive
                   regularization. We benchmark our methods on the ILSVRC 2012
                   classification challenge validation set demonstrate
                   substantial gains over the state of the art: 21.2\% top-1
                   and 5.6\% top-5 error for single frame evaluation using a
                   network with a computational cost of 5 billion multiply-adds
                   per inference and with using less than 25 million
                   parameters. With an ensemble of 4 models and multi-crop
                   evaluation, we report 3.5\% top-5 error on the validation
                   set (3.6\% error on the test set) and 17.3\% top-1 error on
                   the validation set.",
  month         =  dec,
  year          =  2015,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1512.00567"
}
@ARTICLE{unbiasedFID,
  title         = "Effectively Unbiased {FID} and Inception Score and where to
                   find them",
  author        = "Chong, Min Jin and Forsyth, David",
  abstract      = "This paper shows that two commonly used evaluation metrics
                   for generative models, the Fr\textbackslash'echet Inception
                   Distance (FID) and the Inception Score (IS), are biased --
                   the expected value of the score computed for a finite sample
                   set is not the true value of the score. Worse, the paper
                   shows that the bias term depends on the particular model
                   being evaluated, so model A may get a better score than
                   model B simply because model A's bias term is smaller. This
                   effect cannot be fixed by evaluating at a fixed number of
                   samples. This means all comparisons using FID or IS as
                   currently computed are unreliable. We then show how to
                   extrapolate the score to obtain an effectively bias-free
                   estimate of scores computed with an infinite number of
                   samples, which we term $\overline\{\textrm\{FID\}\}_\infty$
                   and $\overline\{\textrm\{IS\}\}_\infty$. In turn, this
                   effectively bias-free estimate requires good estimates of
                   scores with a finite number of samples. We show that using
                   Quasi-Monte Carlo integration notably improves estimates of
                   FID and IS for finite sample sets. Our extrapolated scores
                   are simple, drop-in replacements for the finite sample
                   scores. Additionally, we show that using low discrepancy
                   sequence in GAN training offers small improvements in the
                   resulting generator.",
  month         =  nov,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "1911.07023"
}
@ARTICLE{PrecesionRecall,
  title         = "Improved precision and recall metric for assessing
                   generative models",
  author        = "Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine,
                   Samuli and Lehtinen, Jaakko and Aila, Timo",
  abstract      = "The ability to automatically estimate the quality and
                   coverage of the samples produced by a generative model is a
                   vital requirement for driving algorithm research. We present
                   an evaluation metric that can separately and reliably
                   measure both of these aspects in image generation tasks by
                   forming explicit, non-parametric representations of the
                   manifolds of real and generated data. We demonstrate the
                   effectiveness of our metric in StyleGAN and BigGAN by
                   providing several illustrative examples where existing
                   metrics yield uninformative or contradictory results.
                   Furthermore, we analyze multiple design variants of StyleGAN
                   to better understand the relationships between the model
                   architecture, training methods, and the properties of the
                   resulting sample distribution. In the process, we identify
                   new variants that improve the state-of-the-art. We also
                   perform the first principled analysis of truncation methods
                   and identify an improved method. Finally, we extend our
                   metric to estimate the perceptual quality of individual
                   samples, and use this to study latent space interpolations.",
  month         =  apr,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1904.06991"
}
@ARTICLE{RarityScore,
  title         = "Rarity score : A new metric to evaluate the uncommonness of
                   synthesized images",
  author        = "Han, Jiyeon and Choi, Hwanil and Choi, Yunjey and Kim, Junho
                   and Ha, Jung-Woo and Choi, Jaesik",
  abstract      = "Evaluation metrics in image synthesis play a key role to
                   measure performances of generative models. However, most
                   metrics mainly focus on image fidelity. Existing diversity
                   metrics are derived by comparing distributions, and thus
                   they cannot quantify the diversity or rarity degree of each
                   generated image. In this work, we propose a new evaluation
                   metric, called `rarity score', to measure the individual
                   rarity of each image synthesized by generative models. We
                   first show empirical observation that common samples are
                   close to each other and rare samples are far from each other
                   in nearest-neighbor distances of feature space. We then use
                   our metric to demonstrate that the extent to which different
                   generative models produce rare images can be effectively
                   compared. We also propose a method to compare rarities
                   between datasets that share the same concept such as
                   CelebA-HQ and FFHQ. Finally, we analyze the use of metrics
                   in different designs of feature spaces to better understand
                   the relationship between feature spaces and resulting sparse
                   images. Code will be publicly available online for the
                   research community.",
  month         =  jun,
  year          =  2022,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2206.08549"
}
@ARTICLE{DIffModels,
  title         = "Exposing flaws of generative model evaluation metrics and
                   their unfair treatment of diffusion models",
  author        = "Stein, George and Cresswell, Jesse C and Hosseinzadeh, Rasa
                   and Sui, Yi and Ross, Brendan Leigh and Villecroze, Valentin
                   and Liu, Zhaoyan and Caterini, Anthony L and Taylor, J Eric
                   T and Loaiza-Ganem, Gabriel",
  abstract      = "We systematically study a wide variety of generative models
                   spanning semantically-diverse image datasets to understand
                   and improve the feature extractors and metrics used to
                   evaluate them. Using best practices in psychophysics, we
                   measure human perception of image realism for generated
                   samples by conducting the largest experiment evaluating
                   generative models to date, and find that no existing metric
                   strongly correlates with human evaluations. Comparing to 17
                   modern metrics for evaluating the overall performance,
                   fidelity, diversity, rarity, and memorization of generative
                   models, we find that the state-of-the-art perceptual realism
                   of diffusion models as judged by humans is not reflected in
                   commonly reported metrics such as FID. This discrepancy is
                   not explained by diversity in generated samples, though one
                   cause is over-reliance on Inception-V3. We address these
                   flaws through a study of alternative self-supervised feature
                   extractors, find that the semantic information encoded by
                   individual networks strongly depends on their training
                   procedure, and show that DINOv2-ViT-L/14 allows for much
                   richer evaluation of generative models. Next, we investigate
                   data memorization, and find that generative models do
                   memorize training examples on simple, smaller datasets like
                   CIFAR10, but not necessarily on more complex datasets like
                   ImageNet. However, our experiments show that current metrics
                   do not properly detect memorization: none in the literature
                   is able to separate memorization from other phenomena such
                   as underfitting or mode shrinkage. To facilitate further
                   development of generative models and their evaluation we
                   release all generated image datasets, human evaluation data,
                   and a modular library to compute 17 common metrics for 9
                   different encoders at
                   https://github.com/layer6ai-labs/dgm-eval.",
  month         =  jun,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.LG",
  eprint        = "2306.04675"
}
@ARTICLE{Coverage_Density,
  title         = "Reliable fidelity and diversity metrics for generative
                   models",
  author        = "Naeem, Muhammad Ferjad and Oh, Seong Joon and Uh, Youngjung
                   and Choi, Yunjey and Yoo, Jaejun",
  abstract      = "Devising indicative evaluation metrics for the image
                   generation task remains an open problem. The most widely
                   used metric for measuring the similarity between real and
                   generated images has been the Fr\textbackslash'echet
                   Inception Distance (FID) score. Because it does not
                   differentiate the fidelity and diversity aspects of the
                   generated images, recent papers have introduced variants of
                   precision and recall metrics to diagnose those properties
                   separately. In this paper, we show that even the latest
                   version of the precision and recall metrics are not reliable
                   yet. For example, they fail to detect the match between two
                   identical distributions, they are not robust against
                   outliers, and the evaluation hyperparameters are selected
                   arbitrarily. We propose density and coverage metrics that
                   solve the above issues. We analytically and experimentally
                   show that density and coverage provide more interpretable
                   and reliable signals for practitioners than the existing
                   metrics. Code:
                   https://github.com/clovaai/generative-evaluation-prdc.",
  month         =  feb,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2002.09797"
}
@ARTICLE{Precision_recall,
  title         = "Improved precision and recall metric for assessing
                   generative models",
  author        = "Kynk{\"a}{\"a}nniemi, Tuomas and Karras, Tero and Laine,
                   Samuli and Lehtinen, Jaakko and Aila, Timo",
  abstract      = "The ability to automatically estimate the quality and
                   coverage of the samples produced by a generative model is a
                   vital requirement for driving algorithm research. We present
                   an evaluation metric that can separately and reliably
                   measure both of these aspects in image generation tasks by
                   forming explicit, non-parametric representations of the
                   manifolds of real and generated data. We demonstrate the
                   effectiveness of our metric in StyleGAN and BigGAN by
                   providing several illustrative examples where existing
                   metrics yield uninformative or contradictory results.
                   Furthermore, we analyze multiple design variants of StyleGAN
                   to better understand the relationships between the model
                   architecture, training methods, and the properties of the
                   resulting sample distribution. In the process, we identify
                   new variants that improve the state-of-the-art. We also
                   perform the first principled analysis of truncation methods
                   and identify an improved method. Finally, we extend our
                   metric to estimate the perceptual quality of individual
                   samples, and use this to study latent space interpolations.",
  month         =  apr,
  year          =  2019,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "stat.ML",
  eprint        = "1904.06991"
}
@ARTICLE{DINO,
  title         = "Emerging properties in self-supervised vision transformers",
  author        = "Caron, Mathilde and Touvron, Hugo and Misra, Ishan and
                   J{\'e}gou, Herv{\'e} and Mairal, Julien and Bojanowski,
                   Piotr and Joulin, Armand",
  abstract      = "In this paper, we question if self-supervised learning
                   provides new properties to Vision Transformer (ViT) that
                   stand out compared to convolutional networks (convnets).
                   Beyond the fact that adapting self-supervised methods to
                   this architecture works particularly well, we make the
                   following observations: first, self-supervised ViT features
                   contain explicit information about the semantic segmentation
                   of an image, which does not emerge as clearly with
                   supervised ViTs, nor with convnets. Second, these features
                   are also excellent k-NN classifiers, reaching 78.3\% top-1
                   on ImageNet with a small ViT. Our study also underlines the
                   importance of momentum encoder, multi-crop training, and the
                   use of small patches with ViTs. We implement our findings
                   into a simple self-supervised method, called DINO, which we
                   interpret as a form of self-distillation with no labels. We
                   show the synergy between DINO and ViTs by achieving 80.1\%
                   top-1 on ImageNet in linear evaluation with ViT-Base.",
  month         =  apr,
  year          =  2021,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2104.14294"
}
@misc{COCODataset,
  title = {COCO dataset},
  howpublished = {\url{https://cocodataset.org}}
}
@misc{MJHQ30K,
  title = {MJHQ30k dataset},
  howpublished = {\url{https://huggingface.co/datasets/playgroundai/MJHQ-30K}}
}

@ARTICLE{Image_reward,
  title         = "{ImageReward}: Learning and evaluating human preferences for
                   text-to-image generation",
  author        = "Xu, Jiazheng and Liu, Xiao and Wu, Yuchen and Tong, Yuxuan
                   and Li, Qinkai and Ding, Ming and Tang, Jie and Dong, Yuxiao",
  abstract      = "We present a comprehensive solution to learn and improve
                   text-to-image models from human preference feedback. To
                   begin with, we build ImageReward -- the first
                   general-purpose text-to-image human preference reward model
                   -- to effectively encode human preferences. Its training is
                   based on our systematic annotation pipeline including rating
                   and ranking, which collects 137k expert comparisons to date.
                   In human evaluation, ImageReward outperforms existing
                   scoring models and metrics, making it a promising automatic
                   metric for evaluating text-to-image synthesis. On top of it,
                   we propose Reward Feedback Learning (ReFL), a direct tuning
                   algorithm to optimize diffusion models against a scorer.
                   Both automatic and human evaluation support ReFL's
                   advantages over compared methods. All code and datasets are
                   provided at
                   \textbackslashurl\{https://github.com/THUDM/ImageReward\}.",
  month         =  apr,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2304.05977"
}
@ARTICLE{HPSv2,
  title         = "Human Preference Score v2: A solid benchmark for evaluating
                   human preferences of text-to-image synthesis",
  author        = "Wu, Xiaoshi and Hao, Yiming and Sun, Keqiang and Chen,
                   Yixiong and Zhu, Feng and Zhao, Rui and Li, Hongsheng",
  abstract      = "Recent text-to-image generative models can generate
                   high-fidelity images from text inputs, but the quality of
                   these generated images cannot be accurately evaluated by
                   existing evaluation metrics. To address this issue, we
                   introduce Human Preference Dataset v2 (HPD v2), a
                   large-scale dataset that captures human preferences on
                   images from a wide range of sources. HPD v2 comprises
                   798,090 human preference choices on 433,760 pairs of images,
                   making it the largest dataset of its kind. The text prompts
                   and images are deliberately collected to eliminate potential
                   bias, which is a common issue in previous datasets. By
                   fine-tuning CLIP on HPD v2, we obtain Human Preference Score
                   v2 (HPS v2), a scoring model that can more accurately
                   predict human preferences on generated images. Our
                   experiments demonstrate that HPS v2 generalizes better than
                   previous metrics across various image distributions and is
                   responsive to algorithmic improvements of text-to-image
                   generative models, making it a preferable evaluation metric
                   for these models. We also investigate the design of the
                   evaluation prompts for text-to-image generative models, to
                   make the evaluation stable, fair and easy-to-use. Finally,
                   we establish a benchmark for text-to-image generative models
                   using HPS v2, which includes a set of recent text-to-image
                   models from the academic, community and industry. The code
                   and dataset is available at https://github.com/tgxs002/HPSv2
                   .",
  month         =  jun,
  year          =  2023,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2306.09341"
}
@ARTICLE{Visual_transformer,
  title         = "An image is worth 16x16 words: Transformers for image
                   recognition at scale",
  author        = "Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov,
                   Alexander and Weissenborn, Dirk and Zhai, Xiaohua and
                   Unterthiner, Thomas and Dehghani, Mostafa and Minderer,
                   Matthias and Heigold, Georg and Gelly, Sylvain and
                   Uszkoreit, Jakob and Houlsby, Neil",
  abstract      = "While the Transformer architecture has become the de-facto
                   standard for natural language processing tasks, its
                   applications to computer vision remain limited. In vision,
                   attention is either applied in conjunction with
                   convolutional networks, or used to replace certain
                   components of convolutional networks while keeping their
                   overall structure in place. We show that this reliance on
                   CNNs is not necessary and a pure transformer applied
                   directly to sequences of image patches can perform very well
                   on image classification tasks. When pre-trained on large
                   amounts of data and transferred to multiple mid-sized or
                   small image recognition benchmarks (ImageNet, CIFAR-100,
                   VTAB, etc.), Vision Transformer (ViT) attains excellent
                   results compared to state-of-the-art convolutional networks
                   while requiring substantially fewer computational resources
                   to train.",
  month         =  oct,
  year          =  2020,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2010.11929"
}
@ARTICLE{DALLE,
  title         = "Zero-shot text-to-image generation",
  author        = "Ramesh, Aditya and Pavlov, Mikhail and Goh, Gabriel and
                   Gray, Scott and Voss, Chelsea and Radford, Alec and Chen,
                   Mark and Sutskever, Ilya",
  abstract      = "Text-to-image generation has traditionally focused on
                   finding better modeling assumptions for training on a fixed
                   dataset. These assumptions might involve complex
                   architectures, auxiliary losses, or side information such as
                   object part labels or segmentation masks supplied during
                   training. We describe a simple approach for this task based
                   on a transformer that autoregressively models the text and
                   image tokens as a single stream of data. With sufficient
                   data and scale, our approach is competitive with previous
                   domain-specific models when evaluated in a zero-shot
                   fashion.",
  month         =  feb,
  year          =  2021,
  copyright     = "http://creativecommons.org/licenses/by/4.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2102.12092"
}
@ARTICLE{SelfEval,
  title         = "{SelfEval}: Leveraging the discriminative nature of
                   generative models for evaluation",
  author        = "Rambhatla, Sai Saketh and Misra, Ishan",
  abstract      = "In this work, we show that text-to-image generative models
                   can be 'inverted' to assess their own text-image
                   understanding capabilities in a completely automated manner.
                   Our method, called SelfEval, uses the generative model to
                   compute the likelihood of real images given text prompts,
                   making the generative model directly applicable to
                   discriminative tasks. Using SelfEval, we repurpose standard
                   datasets created for evaluating multimodal text-image
                   discriminative models to evaluate generative models in a
                   fine-grained manner: assessing their performance on
                   attribute binding, color recognition, counting, shape
                   recognition, spatial understanding. To the best of our
                   knowledge SelfEval is the first automated metric to show a
                   high degree of agreement for measuring text-faithfulness
                   with the gold-standard human evaluations across multiple
                   models and benchmarks. Moreover, SelfEval enables us to
                   evaluate generative models on challenging tasks such as
                   Winoground image-score where they demonstrate competitive
                   performance to discriminative models. We also show severe
                   drawbacks of standard automated metrics such as CLIP-score
                   to measure text faithfulness on benchmarks such as
                   DrawBench, and how SelfEval sidesteps these issues. We hope
                   SelfEval enables easy and reliable automated evaluation for
                   diffusion models.",
  month         =  nov,
  year          =  2023,
  copyright     = "http://arxiv.org/licenses/nonexclusive-distrib/1.0/",
  archivePrefix = "arXiv",
  primaryClass  = "cs.CV",
  eprint        = "2311.10708"
}
