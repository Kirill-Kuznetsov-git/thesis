\begin{abstract}
In this paper, I study the existing metrics for evaluating text-to-image generative models. I divide the current metrics into distance metrics and score metrics. I conduct various experiments in order to deduce the best metric and evaluate metrics in such a way that one metric is considered better than another if it correlates better with human judgement. The main contributions are the derivation of the best distance metric is FD\_DINOv2, the derivation of the best score metrics is ImageReward and the creation of it is own metric COS\_DINOv2, which differs from both distance metrics and score metrics. FD\_DINOv2 is a metric that uses DINOv2 to get embeddings from images and fr√©chet distance to calculate the distance between two distributions - embeddings of real and generated images. ImageReward is an improved version of the CLIPScore metric. COS\_DINOv2 is a metric that uses DINOv2 to get embeddings from images and for each pair of embeddings calculate a score that shows how similarity of the embeddings of the real and generated images, respectively showing how similar the real and generated images are.
\end{abstract}